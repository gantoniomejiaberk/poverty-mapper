{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of .json files in \"results\" directory\n",
    "s3 = boto3.client(\"s3\")\n",
    "    \n",
    "bucket = \"w210-poverty-mapper\"\n",
    "directory_path = \"modeling/results/\"\n",
    "contents = s3.list_objects(Bucket=bucket, Prefix=directory_path)['Contents']\n",
    "\n",
    "directory_items = []\n",
    "\n",
    "for f in contents:\n",
    "    directory_items.append(f[\"Key\"])\n",
    "\n",
    "result_jsons = [x for x in directory_items if \".json\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:54: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:56: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:91: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:72: RuntimeWarning: invalid value encountered in true_divide\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:104: RuntimeWarning: invalid value encountered in true_divide\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:92: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:90: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "# Create emtpy df to capture results\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Create counter for file ID\n",
    "count = 0\n",
    "\n",
    "# Grab results from json files\n",
    "for file in result_jsons:\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "    response = s3.get_object(Bucket = bucket, Key = file)\n",
    "    content = response['Body']\n",
    "    result = json.loads(content.read())\n",
    "    \n",
    "    # Get dataframe with model spec\n",
    "    spec = pd.DataFrame.from_records([result[\"model_spec_content\"]])\n",
    "    spec = spec[[\"split_name\", \"num_classes\", \"bin_method\", \"pretrained\", \"freeze_layers\", \"epochs\", \"learning_rate\", \n",
    "                \"gamma\", \"step_size\", \"batch_size\", \"num_workers\"]]\n",
    " \n",
    "    single_metrics_list = [\"test_acc\"]\n",
    "    \n",
    "    for field in single_metrics_list: \n",
    "        spec[field] = result[field]\n",
    "    \n",
    "    # Subset full result dictionary to get dictionaries of metrics\n",
    "    multi_metrics_list = [\"train_losses\", \"train_accs\", \"val_losses\", \"val_accs\"]\n",
    "        \n",
    "    for epoch in range(result[\"model_spec_content\"][\"epochs\"]): \n",
    "        spec[\"epoch\"] = epoch\n",
    "        \n",
    "        # Get metrics with one record per epoch\n",
    "        for field in multi_metrics_list: \n",
    "            spec[field] = result[field][epoch]\n",
    "\n",
    "        spec[\"any_cm_len_invalid\"] = 0\n",
    "\n",
    "        # Process train, val confusion matrices\n",
    "        for cf in [\"train\", \"val\"]:\n",
    "            cf_field = \"{}_cfs\".format(cf)\n",
    "            \n",
    "            # Get number of classes \n",
    "            num_classes = result[\"model_spec_content\"][\"num_classes\"]    \n",
    "\n",
    "            cm = np.array(result[cf_field][epoch])\n",
    "\n",
    "            # Get precision, recall, f1 for binary case\n",
    "            # Note: FP and FN reversed due to order of arguements provided to \n",
    "            # skleran confusion matrix function in trainer.py & zero is positive case\n",
    "            if num_classes == 2:\n",
    "                #print(cm)\n",
    "                if cm.shape[0] == num_classes and cm.shape[1] == num_classes:\n",
    "                    #print(cm)\n",
    "                    precision = cm[0,0]/(cm[0,0] + cm[0,1])\n",
    "                    #print(precision)\n",
    "                    recall = cm[0,0]/(cm[0,0] + cm[1,0])\n",
    "                    #print(recall)\n",
    "                    f1 = (2 * precision * recall) / (precision + recall)\n",
    "                    #print(f1)\n",
    "                    spec[\"{}_precision\".format(cf)] = precision\n",
    "                    spec[\"{}_recall\".format(cf)] = recall\n",
    "                    spec[\"{}_f1\".format(cf)] = f1\n",
    "                else:\n",
    "                    spec[\"any_cm_len_invalid\"] = 1\n",
    "                    spec[\"{}_precision\".format(cf)] = np.nan\n",
    "                    spec[\"{}_recall\".format(cf)] = np.nan\n",
    "                    spec[\"{}_f1\".format(cf)] = np.nan\n",
    "            \n",
    "            # Get per class accuracy for multi-class case\n",
    "            else:   \n",
    "                if cm.shape[0] == num_classes and cm.shape[1] == num_classes:\n",
    "                    accuracies = np.array(result[cf_field][epoch]).diagonal()/np.array(result[cf_field][epoch]).sum(axis=0)\n",
    "                    for acc in range(len(accuracies)): \n",
    "                        spec[\"{}_class_{}_accuracy\".format(cf, acc)] = accuracies[acc]\n",
    "                else:\n",
    "                    spec[\"any_cm_len_invalid\"] = 1\n",
    "                    for acc in range(num_classes): \n",
    "                        spec[\"{}_class_{}_accuracy\".format(cf, acc)] = np.nan\n",
    "        \n",
    "        # Process test confusion matrix\n",
    "        # Note: FP and FN reversed due to order of arguements provided to \n",
    "        # skleran confusion matrix function in trainer.py & zero is positive case\n",
    "        cf = \"test\"\n",
    "        cf_field = \"test_cf\".format(cf)\n",
    "        \n",
    "        cm = np.array(result[cf_field])\n",
    "\n",
    "        if num_classes == 2:\n",
    "            if cm.shape[0] == num_classes and cm.shape[1] == num_classes:\n",
    "                precision = cm[0,0]/(cm[0,0] + cm[0,1])\n",
    "                recall = cm[0,0]/(cm[0,0] + cm[1,0])\n",
    "                f1 = (2 * precision * recall) / (precision + recall)\n",
    "                spec[\"{}_precision\".format(cf)] = precision\n",
    "                spec[\"{}_recall\".format(cf)] = recall\n",
    "                spec[\"{}_f1\".format(cf)] = f1\n",
    "            else:\n",
    "                spec[\"any_cm_len_invalid\"] = 1\n",
    "                spec[\"{}_precision\".format(cf)] = np.nan\n",
    "                spec[\"{}_recall\".format(cf)] = np.nan\n",
    "                spec[\"{}_f1\".format(cf)] = np.nan\n",
    "                \n",
    "        else: \n",
    "            if cm.shape[0] == num_classes and cm.shape[1] == num_classes:\n",
    "                accuracies = np.array(cm).diagonal()/np.array(cm).sum(axis=0)\n",
    "                for acc in range(len(accuracies)): \n",
    "                    spec[\"{}_class_{}_accuracy\".format(cf, acc)] = accuracies[acc] \n",
    "            else:\n",
    "                spec[\"any_cm_len_invalid\"] = 1\n",
    "                for acc in range(num_classes): \n",
    "                    spec[\"{}_class_{}_accuracy\".format(cf, acc)] = np.nan\n",
    "                \n",
    "        spec[\"id\"] = count\n",
    "    \n",
    "        results_df = pd.concat([results_df, spec], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count invalid confusion matrices\n",
    "sum(results_df[\"any_cm_len_invalid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split_name</th>\n",
       "      <th>num_classes</th>\n",
       "      <th>bin_method</th>\n",
       "      <th>pretrained</th>\n",
       "      <th>freeze_layers</th>\n",
       "      <th>epochs</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>gamma</th>\n",
       "      <th>step_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>...</th>\n",
       "      <th>id</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>leave_one_out_tajikistan_5k_50d</td>\n",
       "      <td>2</td>\n",
       "      <td>across</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>no</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>448</td>\n",
       "      <td>...</td>\n",
       "      <td>287</td>\n",
       "      <td>0.143113</td>\n",
       "      <td>0.071187</td>\n",
       "      <td>0.095080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>leave_one_out_tajikistan_5k_50d</td>\n",
       "      <td>2</td>\n",
       "      <td>across</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>no</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>448</td>\n",
       "      <td>...</td>\n",
       "      <td>288</td>\n",
       "      <td>0.204713</td>\n",
       "      <td>0.128315</td>\n",
       "      <td>0.157751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>leave_one_out_tajikistan_5k_50d</td>\n",
       "      <td>5</td>\n",
       "      <td>across</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>no</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>448</td>\n",
       "      <td>...</td>\n",
       "      <td>303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>within_country_tajikistan_10k_50d</td>\n",
       "      <td>2</td>\n",
       "      <td>across</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>no</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>448</td>\n",
       "      <td>...</td>\n",
       "      <td>821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>within_country_tajikistan_10k_50d</td>\n",
       "      <td>2</td>\n",
       "      <td>across</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>no</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>448</td>\n",
       "      <td>...</td>\n",
       "      <td>821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>within_country_tajikistan_5k_50d</td>\n",
       "      <td>5</td>\n",
       "      <td>across</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>no</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>448</td>\n",
       "      <td>...</td>\n",
       "      <td>888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>within_country_tajikistan_5k_50d</td>\n",
       "      <td>5</td>\n",
       "      <td>across</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>no</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>448</td>\n",
       "      <td>...</td>\n",
       "      <td>888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>within_country_tajikistan_5k_50d</td>\n",
       "      <td>5</td>\n",
       "      <td>across</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>no</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>448</td>\n",
       "      <td>...</td>\n",
       "      <td>888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>within_country_tajikistan_5k_50d</td>\n",
       "      <td>5</td>\n",
       "      <td>across</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>no</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>448</td>\n",
       "      <td>...</td>\n",
       "      <td>888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>within_country_tajikistan_5k_50d</td>\n",
       "      <td>5</td>\n",
       "      <td>across</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>no</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>448</td>\n",
       "      <td>...</td>\n",
       "      <td>888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>483 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           split_name  num_classes bin_method pretrained  \\\n",
       "0     leave_one_out_tajikistan_5k_50d            2     across   resnet18   \n",
       "0     leave_one_out_tajikistan_5k_50d            2     across   resnet18   \n",
       "0     leave_one_out_tajikistan_5k_50d            5     across   resnet18   \n",
       "0   within_country_tajikistan_10k_50d            2     across   resnet18   \n",
       "0   within_country_tajikistan_10k_50d            2     across   resnet18   \n",
       "..                                ...          ...        ...        ...   \n",
       "0    within_country_tajikistan_5k_50d            5     across   resnet18   \n",
       "0    within_country_tajikistan_5k_50d            5     across   resnet18   \n",
       "0    within_country_tajikistan_5k_50d            5     across   resnet18   \n",
       "0    within_country_tajikistan_5k_50d            5     across   resnet18   \n",
       "0    within_country_tajikistan_5k_50d            5     across   resnet18   \n",
       "\n",
       "   freeze_layers  epochs  learning_rate  gamma  step_size  batch_size  ...  \\\n",
       "0             no      20         0.0010    0.1         10         448  ...   \n",
       "0             no      20         0.0010    0.1          5         448  ...   \n",
       "0             no      20         0.0010    0.1         10         448  ...   \n",
       "0             no      10         0.0001    0.1         10         448  ...   \n",
       "0             no      10         0.0001    0.1         10         448  ...   \n",
       "..           ...     ...            ...    ...        ...         ...  ...   \n",
       "0             no      20         0.0010    0.1          5         448  ...   \n",
       "0             no      20         0.0010    0.1          5         448  ...   \n",
       "0             no      20         0.0010    0.1          5         448  ...   \n",
       "0             no      20         0.0010    0.1          5         448  ...   \n",
       "0             no      20         0.0010    0.1          5         448  ...   \n",
       "\n",
       "     id  train_precision  train_recall  train_f1  val_precision  val_recall  \\\n",
       "0   287         0.143113      0.071187  0.095080            NaN         NaN   \n",
       "0   288         0.204713      0.128315  0.157751            NaN         NaN   \n",
       "0   303              NaN           NaN       NaN            NaN         NaN   \n",
       "0   821         0.000000           NaN       NaN            NaN         NaN   \n",
       "0   821              NaN           NaN       NaN            0.0         NaN   \n",
       "..  ...              ...           ...       ...            ...         ...   \n",
       "0   888              NaN           NaN       NaN            NaN         NaN   \n",
       "0   888              NaN           NaN       NaN            NaN         NaN   \n",
       "0   888              NaN           NaN       NaN            NaN         NaN   \n",
       "0   888              NaN           NaN       NaN            NaN         NaN   \n",
       "0   888              NaN           NaN       NaN            NaN         NaN   \n",
       "\n",
       "    val_f1  test_precision  test_recall  test_f1  \n",
       "0      NaN             0.0          NaN      NaN  \n",
       "0      NaN             0.0          NaN      NaN  \n",
       "0      NaN             NaN          NaN      NaN  \n",
       "0      NaN             NaN          NaN      NaN  \n",
       "0      NaN             NaN          NaN      NaN  \n",
       "..     ...             ...          ...      ...  \n",
       "0      NaN             NaN          NaN      NaN  \n",
       "0      NaN             NaN          NaN      NaN  \n",
       "0      NaN             NaN          NaN      NaN  \n",
       "0      NaN             NaN          NaN      NaN  \n",
       "0      NaN             NaN          NaN      NaN  \n",
       "\n",
       "[483 rows x 43 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect invalid confusion matrices\n",
    "results_df[results_df[\"any_cm_len_invalid\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add split category column\n",
    "results_df[\"split\"] = results_df.apply(lambda row: \"_\".join(row[\"split_name\"].split(\"_\")[:-2]), axis=1)\n",
    "\n",
    "def extract_split_group(string): \n",
    "    if string.startswith(\"leave_one_out\"):\n",
    "        return \"leave_one_out\"\n",
    "    elif string.startswith(\"within_country\"):\n",
    "        return \"within_country\"\n",
    "    else: \n",
    "        return \"similar geography\"\n",
    "\n",
    "results_df[\"split_group\"] = results_df.apply(lambda row: extract_split_group(row[\"split_name\"]), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add indicator for top val accuracy by group\n",
    "grouped = results_df.groupby([\"split\", \"num_classes\", \"bin_method\", \"id\"])\n",
    "grouped = grouped[\"val_f1\"].max()\n",
    "grouped = grouped.reset_index()\n",
    "grouped = grouped.rename(columns={\"val_f1\": \"max_val_f1\"})\n",
    "\n",
    "def get_top_n(col, in_name, out_name, k):\n",
    "    values = sorted(col[in_name].tolist(), reverse=True)\n",
    "    thresh = values[min(k, len(values))-1]\n",
    "    col[out_name] = col.apply(lambda row: 1 if row[in_name] >= thresh else 0, axis=1)\n",
    "    return col\n",
    "\n",
    "grouped = grouped.groupby([\"split\", \"num_classes\", \"bin_method\"]).apply(lambda x: get_top_n(x, \"max_val_f1\", \"top_n_max_val_f1\", 3))\n",
    "joined = results_df.merge(grouped, on=[\"split\", \"num_classes\", \"bin_method\", \"id\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write combined results as csv\n",
    "joined.to_csv(\"s3://w210-poverty-mapper/modeling/results/combined_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['leave_one_out_tajikistan_5k_50d',\n",
       "       'within_country_tajikistan_10k_50d',\n",
       "       'within_country_tajikistan_5k_50d'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print unique split names for invalid confusion matrices\n",
    "joined[joined[\"any_cm_len_invalid\"] == 1][\"split_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
